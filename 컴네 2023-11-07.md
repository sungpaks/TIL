Congestion Control 했었죠

flow control은 receiver의 윈도우를 고려함. congestion은 네트워크의 상황을 고려함

어떻게 congestion이 발생했음을 알고, 어떻게 대처? (전송을 줄이겠지만, 어떻게 줄인다?)

sender 입장에선 congestion상황에 대해 잘 알 수 없다

시나리오 1, 2 : 같은 그림에서, retransmission수행여부와 라우터 버퍼사이즈 등의 차이가 있음

시나리오 3 : 좀 더 복잡. 여러 호스트가 존재하며 이 호스트의 다운스트림 업스트림 path까지 고려해야.
	throughput 

congestion의 결과 
![](https://i.imgur.com/5M92hb6.png)
- bottleneck링크의 capacity 이상의 throughput을 가질 수 없음
- capacity에 접근할수록 delay는 쭉 증가
- 재전송하느라 throughput낭비
- 추가로 throughput낭비 : 타이머로 loss를 예측하는데, 불필요한 재전송 가능성이 있음
- 여러 호스트가 up/downstream링크를 공유하는 상황에서, throughput이 0에 가까운 값으로 떨어져버릴 수 있다. 

tcp : end-end congestion control : no explicit feedback from network. 타이머나 3dupACK로 추론

근데 이런 경우도 있음 : tcp ecn : explicit congestion notification : 라우터가 자신 버퍼상황에 따라 congestion상황을 알려준다

- AIMD : 서서히 증가, 뚝 감소
	linear하게 증가하다가, loss가 발생하는 등 congestion상황이면 반으로 줄인다
	Multiplicative decrease : sending rate을 줄일 때,
	- Reno : 반으로 줄임
	- Tahoe : 윈도우사이즈 1까지 초기화
	왜 AIMD?
	- 네트워크에서 관리되지 않고, 자체적으로 loss 등을 감지하여 조절하므로 distributed, asynchronous(비동기적 : 동기화할 필요 없고 자체적임)
	
- cwnd 윈도우사이즈 내에, 보내고 아직 ACK 안 받은 구간, 아직 보내지 않은 구간 포함
	- TCP rate ~= cwnd/RTT 

- slow start : congestion이 아닌 상황에서, 1 ,2, 4, 8, ... 이렇게 cwnd증가 : 지수적 증가
	이 때 ssthresh를 지나면 congestion avoidance : 하나의 RTT마다 1MSS만 증가

finite state machine "잘 알고 있어야 하는데 "
Reno버전임. (recovery과정이 있고, 윈도우 사이즈 줄일 때는 반으로 줄임)
![](https://i.imgur.com/WKP01cJ.png)
timeout : 다시 slowstart로 돌아간다
window사이즈가 ssthresh에 도달하면, congestion avoidance로 간다
3dupACK : fast recovery로
- congestion avoidance, fast recovery


최근 많이 deploy되는 TCP CUBIC (원래는 Reno가 대세였다가 이거로 바뀜)
CUBIC : Reno보다 더 큰 window size를 유지하며 증가시킬 수 있음. max 근처까지 빠르게 증가시키다가, 나중엔 느리게
어케함? 어느정도의 개념만
	=> 시간 사이의 거리 세제곱을 이용함
- congestion 상황이 좋아져서 $t_4$에서는 새 max를 찾게 되는 경우 : 다시 새 max를 찾아 빠르게 증가됨

bottleneck link : 병목링크에 따라 congestion이 결정됩니다 (throughput)
	병목링크의 버퍼가 어느정도 찼는지. 이게 또 어느정도는 채워야 할 것이며 그렇다고 다 채울 수도 없는 노릇임
	=> loss가 발생하지 않는 선에서 많이 쓰는게 목표임
	=> delay-based TCP . 원래는 loss 발생 후 대응했지만, *delay*를 측정하여 bottleneck link의 버퍼가 어느정도 유지되도록 한다
		RTT측정, $RTT_{min}$(가장 최상인 경우)를 이용하여, $cwnd/RTT_{min}$ 를 기준으로 함
		throughput을 측정하고, 그 기준값에 가까우면 원활하고, 기준값에 매우 못미치면 혼잡
	이 방식을 쓰는게 TCP vegas
	또 구글에서는 이 방식을 BBR이라는 프로토콜로 배포하여 사용함

ECN : Explicit Congestion Notification : 네트워크에서 직접 혼잡상황을 알려줌. 
network operator 구현이슈 정책이슈.. 어쩌구
IP헤더 ToS 필드의 비트 두 개로 상황을 수신자에게 알린다 (라우터는 network layer까지만 올라가므로, IP헤더에 넣을 수밖에 없음)
수신자는 이걸 받고, 이에 대한 ACK를 TCP헤더에 포함하여 보내 송신자에게 보낸다 (`ECE=1`필드)
기본적으로 어떻게 동작하는지 정도 . 

TCP fairness : 여러 사용자가 동일 수준의 capacity를 가져가야 공평하고 문제없겠죠?
	K개 호스트일 때, 평균 R/K의 rate을 가져가야
- 두 사용자, 링크capacity는 R일 때, 또한 같은 RTT이고 congestion avoidance 등의 방식이 동일할 때 (slow start는 고려하지 않음 : 지수승으로 증가하므로 그 구간이 길지 않다)
	=> 공평한 구간인 점선쪽으로 점근적으로 가야 함
	늘리고 줄이고.. 어디서 시작하든, 점진적 증가하다가 반절 줄고를 반복하면서 점점 점선에 가까워진다
문제가 생기는 경우 :
- UDP와 함께 사용하는 경우 : 예를 들어 UDP5명, TCP5명이라면
	- TCP만 rate을 줄이는 과정이 있다 (TCP가 불리할 수 있음)
- 하나의 application이 TCP 병렬 생성하는 경우 : 예를 들어, 9개 연결이 있는 상황에서,
	- 새 application이 하나의 TCP를 열면 : R/10을 가져감
	- 새 application이 11개 TCP를 열면 : R/2를 가져가버림

요구사항이 다양해지니까 application도 다양해진다..
- Long, fat pipes : 손실이 자주 일어남
- Wireless : noise에 의한 loss를 congestion으로 간주할 수 있음
- Long-delay : RTT가 길다
- latency에 민감한 경우
- 타입에 따라 traffic 우선순위.. 
새로운 요구사항마다 새로운 버전을 개발해버릴 수도 없고

원래는
HTTP/2 -> TLS -> TCP -> IP
요즘 이렇게도 함 :
HTTP/2(slimmed) -> QUIC -> UDP -> IP
- QUIC : Quick UDP Internect Connection. : 연결 + stream으로 구성된다는 특징
	- error and congestion control
	- connection establishment
- TCP handshake, TLS handshake하느라 2RTT를 썼었는데, QUIC handshake 한번만 (1RTT)하게 됨
- 재전송하느라 기다릴 필요 없고, 각 스트림이 병렬적으로 존재하므로 오류가 난 스트림 옆에서는 계속 전송이 이루어질 수 있음